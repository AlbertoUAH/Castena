{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVLsSpE8M-ls"
      },
      "source": [
        "# Castena - Chatbot for multilingual podcasts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RRYSu48huSUW"
      },
      "outputs": [],
      "source": [
        "!pip install langchain huggingface_hub tiktoken -q\n",
        "!pip install chromadb -q\n",
        "!pip install PyPDF2 pypdf sentence_transformers -q\n",
        "!pip install -U together -q\n",
        "!pip install -U FlagEmbedding -q\n",
        "!pip install googletrans==3.1.0a0 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RTYSw4tMO3Mu"
      },
      "outputs": [],
      "source": [
        "# -- Import libraries\n",
        "from   typing                      import Any, Dict, List, Mapping, Optional\n",
        "from   pydantic                    import Extra, Field, root_validator\n",
        "from   langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from   langchain.memory            import ConversationBufferWindowMemory\n",
        "from   langchain.llms.base         import LLM\n",
        "from   langchain.llms.utils        import enforce_stop_tokens\n",
        "from   langchain.chains.llm        import LLMChain\n",
        "from   langchain.utils             import get_from_dict_or_env\n",
        "from   googletrans                 import Translator\n",
        "from   langchain.vectorstores      import Chroma\n",
        "from   langchain.text_splitter     import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from   langchain.chains            import RetrievalQA, ReduceDocumentsChain, MapReduceDocumentsChain\n",
        "from   langchain.document_loaders  import TextLoader, DirectoryLoader\n",
        "from   langchain.embeddings        import HuggingFaceEmbeddings\n",
        "from   langchain.prompts           import PromptTemplate\n",
        "from   langchain.schema            import prompt\n",
        "from   langchain.chains.mapreduce  import MapReduceChain\n",
        "from   langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "import pandas                      as pd\n",
        "import logging\n",
        "import together\n",
        "import textwrap\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvIjaK53dP5l"
      },
      "source": [
        "# Setup API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOGETHER_API_KEY\"] = \"6101599d6e33e3bda336b8d007ca22e35a64c72cfd52c2d8197f663389fc50c5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x8wMSWsXCSY"
      },
      "source": [
        "# Pre-process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KWsdieoO4k29"
      },
      "outputs": [],
      "source": [
        "translator = Translator(service_urls=['translate.googleapis.com'])\n",
        "\n",
        "def translate_text(text, target_lang='en'):\n",
        "    translator = Translator()\n",
        "    translated = translator.translate(str(text), dest=target_lang)\n",
        "    return translated.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmbLNi6ZXEul"
      },
      "outputs": [],
      "source": [
        "# -- Chunk to translate spanish transcripts if necessary (DO NOT RUN IT)\n",
        "transcription_df = pd.read_table('/content/transcriptions.txt', sep='|', header=None)\n",
        "transcription_df.rename(columns={0: 'time', 1: 'speaker', 2: 'transcript'}, inplace=True)\n",
        "\n",
        "transcription_df['time'] = pd.to_timedelta(transcription_df['time'])\n",
        "transcription_df['speaker_change'] = transcription_df['speaker'] != transcription_df['speaker'].shift()\n",
        "\n",
        "result = transcription_df.groupby(['speaker', transcription_df['speaker_change'].cumsum()]).agg({\\\n",
        "                                                                                                 'time': ['min', 'max'],\n",
        "                                                                                                 'transcript': lambda x: '.'.join(x)\n",
        "                                                                                                })\n",
        "result.columns = result.columns.droplevel()\n",
        "result.columns = ['min_time', 'max_time', 'transcript']\n",
        "result.reset_index(inplace=True)\n",
        "result['min_time'] = result['min_time'].apply(lambda x: str(x).replace('0 days ', ''))\n",
        "result['max_time'] = result['max_time'].apply(lambda x: str(x).replace('0 days ', ''))\n",
        "result['literal_transcript'] = 'Desde el instante ' + result['min_time'] + ' hasta ' + result['max_time'] + ' ' + result['speaker'] + ' dice: \\\"' + result['transcript'] + '\\\"'\n",
        "result['literal_transcript'] = result['literal_transcript'].apply(translate_text)\n",
        "result = result.sort_values('min_time')\n",
        "'\\n\\n'.join(result['literal_transcript'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      },
      "source": [
        "# Setting up Together API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3pqftc7nacA",
        "outputId": "e89fa41d-a008-49d3-b67e-693176a7fc57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'success': True,\n",
              " 'value': 'abf10168c5fbd33e6c11168d6f168c4ffea6476c7ec025da91c157f25d64d8cb'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# set your API key\n",
        "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
        "# list available models and descriptons\n",
        "models = together.Models.list()\n",
        "# set llama LLM\n",
        "together.Models.start(\"togethercomputer/llama-2-7b-chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RgbLVmf-o4j7"
      },
      "outputs": [],
      "source": [
        "class TogetherLLM(LLM):\n",
        "    \"\"\"Together large language models.\"\"\"\n",
        "\n",
        "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
        "    \"\"\"model endpoint to use\"\"\"\n",
        "\n",
        "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
        "    \"\"\"Together API key\"\"\"\n",
        "\n",
        "    temperature: float = 0.7\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    max_tokens: int = 512\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the API key is set.\"\"\"\n",
        "        api_key = get_from_dict_or_env(\n",
        "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
        "        )\n",
        "        values[\"together_api_key\"] = api_key\n",
        "        return values\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"together\"\n",
        "\n",
        "    def clean_duplicates(self, transcription: str) -> str:\n",
        "      lines = transcription.strip().split('\\n')\n",
        "      unique_lines = set()\n",
        "\n",
        "      new_transcription = []\n",
        "\n",
        "      for linea in lines:\n",
        "          if linea not in unique_lines:\n",
        "              new_transcription.append(linea)\n",
        "              unique_lines.add(linea)\n",
        "\n",
        "      # Create new transcription without duplicates\n",
        "      new_transcription = '\\n\\n'.join(new_transcription)\n",
        "      return new_transcription\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call to Together endpoint.\"\"\"\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(prompt,\n",
        "                                          model=self.model,\n",
        "                                          max_tokens=self.max_tokens,\n",
        "                                          temperature=self.temperature,\n",
        "                                          )\n",
        "        text = output['output']['choices'][0]['text']\n",
        "        cleaned_text = self.clean_duplicates(text)\n",
        "        return cleaned_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AnZQpL_IZZZ"
      },
      "source": [
        "# LangChain multi-doc retriever with ChromaDB\n",
        "\n",
        "***Key Points***\n",
        "- Multiple Files - PDFs\n",
        "- ChromaDB\n",
        "- llama-2-7b-chat LLM\n",
        "- BGE Embeddings (newest version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgfdhZ5uRpFn"
      },
      "source": [
        "## Setting up LangChain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UcQKUId3X2M"
      },
      "source": [
        "## Load multiple and process documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PRSeXXc_3Ypj"
      },
      "outputs": [],
      "source": [
        "# Load and process the text files\n",
        "loader = TextLoader('/content/transcriptions.es.en.txt')\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT6KgAIT_BtB",
        "outputId": "1fc6570e-be1e-4930-db33-6dfba0bc3c43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3__nT0D4Fkmg",
        "outputId": "4e170bb4-b6ec-4554-d11a-13d7ce806fc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhs0C0FYASlM"
      },
      "source": [
        "## Load HF BGE Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Emj46ATxtV9C"
      },
      "outputs": [],
      "source": [
        "model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "model_norm = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsYsIy8F4cdm"
      },
      "source": [
        "## create the DB\n",
        "\n",
        " T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Why Chroma instead of FAISS?__\n",
        "\n",
        "Answer:\n",
        "\n",
        "In terms of performance, there is no direct benchmark comparison available between **Chroma** and **FAISS**. This is because **FAISS** is not regularly used as a stand-alone vector database, so it is difficult to compare it directly with **Chroma**.\n",
        "\n",
        "**FAISS** is designed for efficient similarity search, which can be crucial for applications involving large-scale semantic search. However, for a production environment, it may need to be built into a custom container or larger system to support CRUD operations, high availability, horizontal scalability, concurrent access, etc. It is built around an Index object. This object encapsulates the set of database vectors and optionally preprocesses them to make the search efficient. There are many types of indexes, but the simplest version performs a brute force Euclidean (L2) distance search.\n",
        "\n",
        "On the other hand, **Chroma** is designed to run on your machine and was built to handle modern AI workloads, making it suitable for embedding-intensive applications.\n",
        "\n",
        "Therefore, the choice between **Chroma** and **FAISS** depends on your specific use case. If you're looking for a standalone vector database that's easy to set up and use for local development, **Chroma** may be a good choice. If you need a tool for efficient similarity search and dense vector clustering and ready to build additional functionality around it, **FAISS** might be suitable."
      ],
      "metadata": {
        "id": "-NL2melT4xlL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_eTIZwf4Dk2",
        "outputId": "25ad2368-f408-4497-e3fd-d382c2470de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.98 s, sys: 138 ms, total: 3.11 s\n",
            "Wall time: 2.95 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = 'db'\n",
        "## Here is the nmew embeddings being used\n",
        "embedding = model_norm\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siLXR-XT0JoI"
      },
      "source": [
        "## Make a retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "jVWgPJXs1yRq"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ia-4OXa5IeP"
      },
      "source": [
        "## Make a chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "IrVIuygNuBVT"
      },
      "outputs": [],
      "source": [
        "## Default LLaMA-2 prompt style\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are an assistant. Your mission is to provide accurate answers to questions regarding the transcription of a YouTube interview.\n",
        "\n",
        "Be concise and omit disclaimers.\n",
        "\n",
        "Emphasize the importance of accuracy and refrain from making guesses or assumptions.\n",
        "\n",
        "If a question is incoherent, politely explain the issue without directly answering it.\n",
        "\n",
        "Do not add emojis to the response.\n",
        "\n",
        "Always conclude your response with the following text  at the end: 'Is there anything else I can assist you with?'\n",
        "\"\"\"\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "vVKwoAnpuD79",
        "outputId": "c32d7cf7-06c4-4f99-b1b9-61c122904a36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[INST]<<SYS>>\\nYou are an assistant. Your mission is to provide accurate answers to questions regarding the transcription of a YouTube interview.\\n\\nBe concise and omit disclaimers.\\n\\nEmphasize the importance of accuracy and refrain from making guesses or assumptions.\\n\\nIf a question is incoherent, politely explain the issue without directly answering it.\\n\\nDo not add emojis to the response.\\n\\nAlways conclude your response with the following text at the end: 'Is there anything else I can assist you with?'\\n\\n<</SYS>>\\n\\nCONTEXT:/n/n {context}/n\\n\\nQuestion: {question}[/INST]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "sys_prompt = \"\"\"\\\n",
        "You are an assistant. Your mission is to provide accurate answers to questions regarding the transcription of a YouTube interview.\n",
        "\n",
        "Be concise and omit disclaimers.\n",
        "\n",
        "Emphasize the importance of accuracy and refrain from making guesses or assumptions.\n",
        "\n",
        "If a question is incoherent, politely explain the issue without directly answering it.\n",
        "\n",
        "Do not add emojis to the response.\n",
        "\n",
        "Always conclude your response with the following text at the end: 'Is there anything else I can assist you with?'\n",
        "\"\"\"\n",
        "\n",
        "instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "get_prompt(instruction, sys_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "dCtX_DK9S-K0"
      },
      "outputs": [],
      "source": [
        "llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-7b-chat\",\n",
        "    temperature = 0.0,\n",
        "    max_tokens = 1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "1LmgdDIfsjXp"
      },
      "outputs": [],
      "source": [
        "prompt_template = get_prompt(instruction, sys_prompt)\n",
        "\n",
        "llama_prompt = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "ts3sZb7fsoMN"
      },
      "outputs": [],
      "source": [
        "# -- NOTE: in case we want to add memory, 'history' field must be added to the prompt\n",
        "# HISTORY:/n/n {history}/n\n",
        "#memory = ConversationBufferWindowMemory(k=1, memory_key=\"history\", input_key=\"question\")\n",
        "#chain_type_kwargs = {\"prompt\": llama_prompt, \"memory\": memory}\n",
        "chain_type_kwargs = {\"prompt\": llama_prompt}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "MGx8XblM4shW"
      },
      "outputs": [],
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type=\"stuff\",\n",
        "                                       retriever=retriever,\n",
        "                                       chain_type_kwargs=chain_type_kwargs,\n",
        "                                       return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "LZEo26mw8e5k"
      },
      "outputs": [],
      "source": [
        "## Cite sources\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "  response = llm_response['result']\n",
        "  return wrap_text_preserve_newlines(translate_text(response, target_lang='es'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKfX4vX-5RFT",
        "outputId": "51d46adc-5f0e-4d43-9f34-f0f977a42687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El entrevistado destaca la importancia de ser útil, tener criterio propio, ser crítico y cuidar de uno mismo y\n",
            "del propio cuerpo. Creen que estos son los aspectos fundamentales para una persona, y que son imprescindibles\n",
            "para el desarrollo de los jóvenes. El entrevistado también menciona que tienen un enfoque realista y que sus\n",
            "propuestas pueden ser difíciles de implementar en el corto o mediano plazo, pero son idealistas y vale la pena\n",
            "esforzarse por lograrlas. No expresan explícitamente su opinión sobre la juventud, pero su énfasis en el\n",
            "autocuidado y el desarrollo personal sugiere que valoran el bienestar y la agencia de los jóvenes.\n"
          ]
        }
      ],
      "source": [
        "# full example\n",
        "query = \"¿Cuál es la opinión del entrevistado sobre la juventud?\"\n",
        "translated_query = translate_text(query, target_lang='en')\n",
        "llm_response = qa_chain(translated_query)\n",
        "print(process_llm_response(llm_response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4aLRBjEBOW9",
        "outputId": "1f8fe31d-0f7f-4526-8f12-61b5b577c847"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'success': True}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "together.Models.stop(\"togethercomputer/llama-2-7b-chat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D1vy5B2RQo4"
      },
      "source": [
        "# Summary of transcription: map reduce technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "v2k-ry1p3LM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c70f5f4-f9b4-42fd-f6fa-f57438ac90b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2297, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4212, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2290, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2536, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 7571, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2283, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4160, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 5948, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2884, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2799, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4093, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 3226, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 5099, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2106, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 3039, which is longer than the specified 2000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 3145, which is longer than the specified 2000\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/transcriptions.es.en.txt\") as f:\n",
        "    docs = f.read()\n",
        "\n",
        "llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-7b-chat\",\n",
        "    temperature = 0.0,\n",
        "    max_tokens = 1024\n",
        ")\n",
        "\n",
        "# Map\n",
        "map_template = \"\"\"The following is a list of a conversation between two speakers\n",
        "{docs}\n",
        "Please summarise the conversation in detail\n",
        "Summary:\"\"\"\n",
        "map_prompt = PromptTemplate(template=map_template, input_variables=[\"docs\"])\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "# Reduce\n",
        "reduce_template = \"\"\"The following is set of summaries:\n",
        "{doc_summaries}\n",
        "Take these and distill it into a final, consolidated summary of the main themes in detail.\n",
        "Separate every theme by \\n\n",
        "Summary:\"\"\"\n",
        "reduce_prompt = PromptTemplate(template=reduce_template, input_variables=[\"doc_summaries\"])\n",
        "\n",
        "# Run chain\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
        ")\n",
        "\n",
        "# Combines and iteravely reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    verbose=True,\n",
        "    token_max=1024\n",
        ")\n",
        "\n",
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        "    verbose=True\n",
        ")\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\\n\",\n",
        "    chunk_size = 2000,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        "    is_separator_regex = True,\n",
        ")\n",
        "split_docs = text_splitter.create_documents([docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtUyMwGOgtv9",
        "outputId": "0a8e6e16-16cf-42da-c3c0-329e029cbb66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The main themes in the conversation between Roberto Vaquero and the interviewer include:\n",
            "\n",
            "\n",
            "\n",
            "1. The importance of addressing material conditions in building a successful communist society.\n",
            "\n",
            "2. The need for a practical approach to building a communist society, rather than relying solely on intellectual debates or \"woke\" movements.\n",
            "\n",
            "3. Criticism of excessive focus on intellectual debates and \"woke\" movements.\n",
            "\n",
            "4. The importance of doing things for the benefit of what you believe in and respecting oneself and others.\n",
            "\n",
            "5. The need to move beyond just imagining or debating and take action.\n",
            "\n",
            "6. Vaquero's experiences in the military and how they have shaped his political beliefs.\n",
            "\n",
            "7. Frustration with the repetition of mistakes and lack of critical thinking within communist groups.\n",
            "\n",
            "8. Vaquero's active presence on social media platforms, including YouTube and Twitch.\n",
            "\n",
            "These themes highlight Vaquero's commitment to building a practical, revolutionary movement that addresses material conditions and prioritizes action over intellectual debates.\n"
          ]
        }
      ],
      "source": [
        "text_summary = map_reduce_chain.run(split_docs)\n",
        "print(text_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "FvFz3dCzoKNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710aa2a2-e1be-4b5e-cb02-f3718512a31d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'success': True}"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "together.Models.stop(\"togethercomputer/llama-2-7b-chat\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}